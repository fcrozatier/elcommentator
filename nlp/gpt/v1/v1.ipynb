{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#df = pd.read_csv('../../data_clean.csv')\n",
    "df = pd.read_csv('../ranking/custom_star.csv')\n",
    "df = df.loc[df['custom_star'] == 3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def train_test_split(dataframe, ratio):\n",
    "  # test_length = int(df.shape[0] * ratio)\n",
    "  # test = dataframe['commentaire'][0:test_length]\n",
    "  # train = dataframe['commentaire'][test_length:]\n",
    "  # return train, test\n",
    "\n",
    "  # Shuffle\n",
    "  size = dataframe.shape[0]\n",
    "  test_length = int(size * ratio)\n",
    "  shuffled_data = dataframe.sample(size)\n",
    "\n",
    "  # Split\n",
    "  test = shuffled_data['commentaire'][0:test_length]\n",
    "  train = shuffled_data['commentaire'][test_length:]\n",
    "  return train, test\n",
    "\n",
    "\n",
    "def build_text_files(data_list, dest_path):\n",
    "  # f = open(dest_path, 'w')\n",
    "  # f.write(' '.join(data_list))\n",
    "  processed_list = list(map(lambda x: x.strip(), data_list ))\n",
    "  with open(dest_path, 'w') as f:\n",
    "    for comment in processed_list:\n",
    "      f.write(comment + '\\n')\n",
    "\n",
    "train, test = train_test_split(df, ratio=0.15) \n",
    "\n",
    "\n",
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')\n",
    "\n",
    "print(\"Train dataset length: \"+str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train dataset length: 2553\n",
      "Test dataset length: 450\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"antoiloui/belgpt2\")\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"antoiloui/belgpt2\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-custom-score-3\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved \n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:898: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 994\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [57:43<00:00, 26.93s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [57:43<00:00, 36.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train_runtime': 3463.844, 'train_samples_per_second': 0.861, 'train_steps_per_second': 0.028, 'train_loss': 2.4611204465230307, 'epoch': 3.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=2.4611204465230307, metrics={'train_runtime': 3463.844, 'train_samples_per_second': 0.861, 'train_steps_per_second': 0.028, 'train_loss': 2.4611204465230307, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "trainer.save_model()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving model checkpoint to ./gpt2-custom-score-3\n",
      "Configuration saved in ./gpt2-custom-score-3/config.json\n",
      "Model weights saved in ./gpt2-custom-score-3/pytorch_model.bin\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "elcommentator = pipeline('text-generation', model='./gpt2-comments', tokenizer='antoiloui/belgpt2')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "elcommentator('Je ne suis')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': 'Je ne suis pas une Ã©lÃ¨ve sÃ©rieuse et attentive. Un bon trimestre. Un bon trimestre. Un bon'}]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}